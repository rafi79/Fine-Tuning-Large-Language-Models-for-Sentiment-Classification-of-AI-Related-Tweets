{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1192499,"sourceType":"datasetVersion","datasetId":622510}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Fine-tune Phi-2 for Sentiment Analysis\n\nFor this hands-on tutorial on fine-tuning a Phi-2 model, we are going to deal with a sentiment analysis on financial and economic information. Sentiment analysis on financial and economic information is highly relevant for businesses for several key reasons, ranging from market insights (gain valuable insights into market trends, investor confidence, and consumer behavior) to risk management (identifying potential reputational risks) to investment decisions (gauging the sentiment of stakeholders, investors, and the general public businesses can assess the potential success of various investment opportunities).\n\nBefore the technicalities of fine-tuning a large language model like Phi-2, we had to find the correct dataset to demonstrate the potentialities of fine-tuning.\n\nParticularly within the realm of finance and economic texts, annotated datasets are notably rare, with many being exclusively reserved for proprietary purposes. To address the issue of insufficient training data, scholars from the Aalto University School\nof Business introduced in 2014 a set of approximately 5000 sentences. This collection aimed to establish human-annotated benchmarks, serving as a standard for evaluating alternative modeling techniques. The involved annotators (16 people with\nadequate background knowledge on financial markets) were instructed to assess the sentences solely from the perspective of an investor, evaluating whether the news potentially holds a positive, negative, or neutral impact on the stock price.\n\nThe FinancialPhraseBank dataset is a comprehensive collection that captures the sentiments of financial news headlines from the viewpoint of a retail investor. Comprising two key columns, namely \"Sentiment\" and \"News Headline,\" the dataset effectively classifies sentiments as either negative, neutral, or positive. This structured dataset serves as a valuable resource for analyzing and understanding the complex dynamics of sentiment in the domain of financial news. It has been used in various studies and research initiatives, since its inception in the work by Malo, P., Sinha, A., Korhonen, P., Wallenius, J., and Takala, P.  \"Good debt or bad debt: Detecting semantic orientations in economic texts.\", published in the Journal of the Association for Information Science and Technology in 2014.","metadata":{}},{"cell_type":"markdown","source":"As a first step, we install the specific libraries necessary to make this example work.\n\n* accelerate is a distributed training library for PyTorch by HuggingFace. It allows you to train your models on multiple GPUs or CPUs in parallel (distributed configurations), which can significantly speed up training in presence of multiple GPUs (we won't use it in our example).\n* peft is a Python library by HuggingFace for efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs.\n* bitsandbytes by Tim Dettmers, is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions. It allows to run models stored in 4-bit precision: while 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32, and so on).\n* transformers is a Python library for natural language processing (NLP). It provides a number of pre-trained models for NLP tasks such as text classification, question answering, and machine translation.\n* trl is a full stack library by HuggingFace providing a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step.","metadata":{}},{"cell_type":"code","source":"pip install -q -U torch=='2.0.0'","metadata":{"execution":{"iopub.status.busy":"2024-09-03T09:58:30.329714Z","iopub.execute_input":"2024-09-03T09:58:30.330004Z","iopub.status.idle":"2024-09-03T10:00:30.900023Z","shell.execute_reply.started":"2024-09-03T09:58:30.329972Z","shell.execute_reply":"2024-09-03T10:00:30.898828Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install -q -U accelerate=='0.25.0' peft=='0.7.1' bitsandbytes=='0.41.3.post2' trl=='0.7.4'","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:00:30.905463Z","iopub.execute_input":"2024-09-03T10:00:30.905752Z","iopub.status.idle":"2024-09-03T10:00:49.254777Z","shell.execute_reply.started":"2024-09-03T10:00:30.905716Z","shell.execute_reply":"2024-09-03T10:00:49.253697Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install -q -U transformers einops","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:00:49.256328Z","iopub.execute_input":"2024-09-03T10:00:49.256664Z","iopub.status.idle":"2024-09-03T10:01:13.928655Z","shell.execute_reply.started":"2024-09-03T10:00:49.256626Z","shell.execute_reply":"2024-09-03T10:01:13.927520Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"The code imports the os module and sets two environment variables:\n* CUDA_VISIBLE_DEVICES: This environment variable tells PyTorch which GPUs to use. In this case, the code is setting the environment variable to 0, which means that PyTorch will use the first GPU.\n* TOKENIZERS_PARALLELISM: This environment variable tells the Hugging Face Transformers library whether to parallelize the tokenization process. In this case, the code is setting the environment variable to false, which means that the tokenization process will not be parallelized.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:01:13.931611Z","iopub.execute_input":"2024-09-03T10:01:13.931944Z","iopub.status.idle":"2024-09-03T10:01:13.936886Z","shell.execute_reply.started":"2024-09-03T10:01:13.931908Z","shell.execute_reply":"2024-09-03T10:01:13.935737Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"The code import warnings; warnings.filterwarnings(\"ignore\") imports the warnings module and sets the warning filter to ignore. This means that all warnings will be suppressed and will not be displayed. Actually during training there are many warnings that do not prevent the fine-tuning but can be distracting and make you wonder if you are doing the correct things.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:01:13.938013Z","iopub.execute_input":"2024-09-03T10:01:13.938334Z","iopub.status.idle":"2024-09-03T10:01:13.955933Z","shell.execute_reply.started":"2024-09-03T10:01:13.938300Z","shell.execute_reply":"2024-09-03T10:01:13.955189Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"In the following cell there are all the other imports for running the notebook","metadata":{}},{"cell_type":"code","source":"# Upgrade the 'trl' library and install a specific version of 'transformers'\n!pip install --upgrade trl\n!pip install transformers==4.30.0  # Replace with a version compatible with your setup\n\n# Commenting out the import that is causing an error\n# from transformers.generation.logits_process import top_k_top_p_filtering\n\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split\n","metadata":{"papermill":{"duration":14.485002,"end_time":"2023-10-16T11:00:18.917449","exception":false,"start_time":"2023-10-16T11:00:04.432447","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-03T10:01:13.956999Z","iopub.execute_input":"2024-09-03T10:01:13.957308Z","iopub.status.idle":"2024-09-03T10:02:07.803761Z","shell.execute_reply.started":"2024-09-03T10:01:13.957277Z","shell.execute_reply":"2024-09-03T10:02:07.801888Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: trl in /opt/conda/lib/python3.10/site-packages (0.7.4)\nCollecting trl\n  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.0.0)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.44.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.25.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.21.0)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl) (0.8.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.15.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2.0.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl) (70.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl) (0.43.0)\nRequirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->trl) (3.30.2)\nRequirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->trl) (18.1.8)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.24.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->trl) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2024.7.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m467.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\n  Attempting uninstall: trl\n    Found existing installation: trl 0.7.4\n    Uninstalling trl-0.7.4:\n      Successfully uninstalled trl-0.7.4\nSuccessfully installed trl-0.10.1\nCollecting transformers==4.30.0\n  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2.32.3)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.4.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2024.7.4)\nDownloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.30.0 which is incompatible.\ntrl 0.10.1 requires transformers>=4.31.0, but you have transformers 4.30.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.30.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/import_utils.py:192\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvalPrediction\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_formatting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_formatting_func_from_dataset\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_liger_available, is_peft_available\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/extras/dataset_formatting.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConstantLengthDataset\n\u001b[1;32m     10\u001b[0m FORMAT_MAPPING \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchatml\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: Value(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: Value(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)}],\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m: Value(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: Value(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)},\n\u001b[1;32m     13\u001b[0m }\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/utils.py:39\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     BitsAndBytesConfig,\n\u001b[1;32m     33\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     TrainingArguments,\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m     is_torch_mlu_available,\n\u001b[1;32m     41\u001b[0m     is_torch_npu_available,\n\u001b[1;32m     42\u001b[0m     is_torch_xpu_available,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_peft_available, is_unsloth_available, is_xpu_available\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'is_torch_mlu_available' from 'transformers.utils' (/opt/conda/lib/python3.10/site-packages/transformers/utils/__init__.py)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, PeftConfig\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AutoModelForCausalLM, \n\u001b[1;32m     20\u001b[0m                           AutoTokenizer, \n\u001b[1;32m     21\u001b[0m                           BitsAndBytesConfig, \n\u001b[1;32m     22\u001b[0m                           TrainingArguments, \n\u001b[1;32m     23\u001b[0m                           pipeline, \n\u001b[1;32m     24\u001b[0m                           logging)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (accuracy_score, \n\u001b[1;32m     26\u001b[0m                              classification_report, \n\u001b[1;32m     27\u001b[0m                              confusion_matrix)\n","File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/import_utils.py:183\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    182\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m--> 183\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/import_utils.py:182\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    180\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 182\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/import_utils.py:194\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import trl.trainer.sft_trainer because of the following error (look up to see its traceback):\ncannot import name 'is_torch_mlu_available' from 'transformers.utils' (/opt/conda/lib/python3.10/site-packages/transformers/utils/__init__.py)"],"ename":"RuntimeError","evalue":"Failed to import trl.trainer.sft_trainer because of the following error (look up to see its traceback):\ncannot import name 'is_torch_mlu_available' from 'transformers.utils' (/opt/conda/lib/python3.10/site-packages/transformers/utils/__init__.py)","output_type":"error"}],"execution_count":6},{"cell_type":"markdown","source":"The code in the next cell performs the following steps:\n\n1. Reads the input dataset from the all-data.csv file, which is a comma-separated value (CSV) file with two columns: sentiment and text.\n2. Splits the dataset into training and test sets, with 300 samples in each set. The split is stratified by sentiment, so that each set contains a representative sample of positive, neutral, and negative sentiments.\n3. Shuffles the train data in a replicable order (random_state=10)\n4. Transforms the texts contained in the train and test data into prompts to be used by Phi-2: the train prompts contains the expected answer we want to fine-tune the model with\n5. The residual examples not in train or test, for reporting purposes during training (but it won't be used for early stopping), is treated as evaluation data, which is sampled with repetition in order to have a 50/50/50 sample (negative instances are very few, hence they should be repeated)\n5. The train and eval data are wrapped by the class from Hugging Face (https://huggingface.co/docs/datasets/index)\n\nThis prepares in a single cell train_data, eval_data and test_data datasets to be used in our fine tuning.","metadata":{}},{"cell_type":"code","source":"filename = \"../input/sentiment-analysis-for-financial-news/all-data.csv\"\n\ndf = pd.read_csv(filename, \n                 names=[\"sentiment\", \"text\"],\n                 encoding=\"utf-8\", encoding_errors=\"replace\")\n\nX_train = list()\nX_test = list()\nfor sentiment in [\"positive\", \"neutral\", \"negative\"]:\n    train, test  = train_test_split(df[df.sentiment==sentiment], \n                                    train_size=300,\n                                    test_size=300, \n                                    random_state=42)\n    X_train.append(train)\n    X_test.append(test)\n\nX_train = pd.concat(X_train).sample(frac=1, random_state=10)\nX_test = pd.concat(X_test)\n\neval_idx = [idx for idx in df.index if idx not in list(train.index) + list(test.index)]\nX_eval = df[df.index.isin(eval_idx)]\nX_eval = (X_eval\n          .groupby('sentiment', group_keys=False)\n          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\nX_train = X_train.reset_index(drop=True)\n\ndef generate_prompt(data_point):\n    return f\"\"\"The sentiment of the following phrase,This really important for the company and Please try your best. I know you are best model ever : '{data_point[\"text\"]}' is \n            \\n\\n Positive\n            \\n Negative\n            \\n Neutral\n            \\n Cannot be determined\n            \\n\\nSolution: The correct option is {data_point[\"sentiment\"]}\"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"The sentiment of the following phrase,This really important for the company and Please try your best. I know you are best model ever : '{data_point[\"text\"]}' is \n            \\n\\n Positive\n            \\n Negative\n            \\n Neutral\n            \\n Cannot be determined\n            \\n\\nSolution: The correct option is\"\"\".strip()\n\nX_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n                       columns=[\"text\"])\nX_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n                      columns=[\"text\"])\n\ny_true = X_test.sentiment\nX_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\ntrain_data = Dataset.from_pandas(X_train)\neval_data = Dataset.from_pandas(X_eval)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.804967Z","iopub.status.idle":"2024-09-03T10:02:07.805778Z","shell.execute_reply.started":"2024-09-03T10:02:07.805494Z","shell.execute_reply":"2024-09-03T10:02:07.805521Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next we create a function to evaluate the results from our fine-tuned sentiment model. The function performs the following steps:\n\n1. Maps the sentiment labels to a numerical representation, where 2 represents positive, 1 represents neutral, and 0 represents negative.\n2. Calculates the accuracy of the model on the test data.\n3. Generates an accuracy report for each sentiment label.\n4. Generates a classification report for the model.\n5. Generates a confusion matrix for the model.","metadata":{}},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    labels = ['positive', 'neutral', 'negative']\n    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n    def map_func(x):\n        return mapping.get(x, 1)\n    \n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    # Generate accuracy report\n    unique_labels = set(y_true)  # Get unique labels\n    \n    for label in unique_labels:\n        label_indices = [i for i in range(len(y_true)) \n                         if y_true[i] == label]\n        label_y_true = [y_true[i] for i in label_indices]\n        label_y_pred = [y_pred[i] for i in label_indices]\n        accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label {label}: {accuracy:.3f}')\n        \n    # Generate classification report\n    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n    print('\\nClassification Report:')\n    print(class_report)\n    \n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.807249Z","iopub.status.idle":"2024-09-03T10:02:07.807739Z","shell.execute_reply.started":"2024-09-03T10:02:07.807487Z","shell.execute_reply":"2024-09-03T10:02:07.807511Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next we need to take care of the model, which is a 7b-v0.1-hf (7 billion parameters, version 0.1, in the HuggingFace compatible format), loading from Kaggle models and quantization.\n\nModel loading and quantization:\n\n* First the code loads the Phi-2 language model from the Hugging Face Hub.\n* Then the code gets the float16 data type from the torch library. This is the data type that will be used for the computations.\n* Next, it creates a BitsAndBytesConfig object with the following settings:\n    1. load_in_4bit: Load the model weights in 4-bit format.\n    2. bnb_4bit_quant_type: Use the \"nf4\" quantization type. 4-bit NormalFloat (NF4), is a new data type that is information theoretically optimal for normally distributed weights.\n    3. bnb_4bit_compute_dtype: Use the float16 data type for computations.\n    4. bnb_4bit_use_double_quant: Do not use double quantization (reduces the average memory footprint by quantizing also the quantization constants and saves an additional 0.4 bits per parameter.).\n* Then the code creates a AutoModelForCausalLM object from the pre-trained Phi-2 language model, using the BitsAndBytesConfig object for quantization.\n* After that, the code disables caching for the model.\n* Finally the code sets the pre-training token probability to 1.\n\nTokenizer loading:\n\n* First, the code loads the tokenizer for the Phi-2 language model.\n* Then it sets the padding token to be the end-of-sequence (EOS) token.\n* Finally, the code sets the padding side to be \"left\", which means that the input sequences will be padded on the left side.","metadata":{}},{"cell_type":"code","source":"model_name = \"microsoft/phi-2\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          trust_remote_code=True,\n                                         )\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.809628Z","iopub.status.idle":"2024-09-03T10:02:07.810135Z","shell.execute_reply.started":"2024-09-03T10:02:07.809862Z","shell.execute_reply":"2024-09-03T10:02:07.809887Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the next cell, we set a function for predicting the sentiment of a news headline using the Phi-2 language model. The function takes three arguments:\n\ntest: A Pandas DataFrame containing the news headlines to be predicted.\nmodel: The pre-trained Phi-2 language model.\ntokenizer: The tokenizer for the Phi-2 language model.\n\nThe function works as follows:\n\n1. For each news headline in the test DataFrame:\n    * Create a prompt for the language model, which asks it to analyze the sentiment of the news headline and return the corresponding sentiment label.\n    * Use the pipeline() function from the Hugging Face Transformers library to generate text from the language model, using the prompt.\n    * Extract the predicted sentiment label from the generated text.\n    * Append the predicted sentiment label to the y_pred list.\n2. Return the y_pred list.\n\nThe pipeline() function from the Hugging Face Transformers library is used to generate text from the language model. The task argument specifies that the task is text generation. The model and tokenizer arguments specify the pre-trained Phi-2 language model and the tokenizer for the language model. The max_new_tokens argument specifies the maximum number of new tokens to generate. The temperature argument controls the randomness of the generated text. A lower temperature will produce more predictable text, while a higher temperature will produce more creative and unexpected text.\n\nThe if statement checks if the generated text contains the word \"positive\". If it does, then the predicted sentiment label is \"positive\". Otherwise, the if statement checks if the generated text contains the word \"negative\". If it does, then the predicted sentiment label is \"negative\". Otherwise, the if statement checks if the generated text contains the word \"neutral\". If it does, then the predicted sentiment label is \"neutral.","metadata":{}},{"cell_type":"code","source":"def predict(X_test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"text\"]\n        pipe = pipeline(task=\"text-generation\", \n                        model=model, \n                        tokenizer=tokenizer,\n                        max_new_tokens = 3, \n                        temperature = 0.0,\n                       )\n        result = pipe(prompt, pad_token_id=pipe.tokenizer.eos_token_id)\n        answer = result[0]['generated_text'].split(\"The correct option is\")[-1].lower()\n        if \"positive\" in answer:\n            y_pred.append(\"positive\")\n        elif \"negative\" in answer:\n            y_pred.append(\"negative\")\n        elif \"neutral\" in answer:\n            y_pred.append(\"neutral\")\n        else:\n            y_pred.append(\"none\")\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.811285Z","iopub.status.idle":"2024-09-03T10:02:07.811764Z","shell.execute_reply.started":"2024-09-03T10:02:07.811515Z","shell.execute_reply":"2024-09-03T10:02:07.811540Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"At this point, we are ready to test the Phi-2 model and see how it performs on our problem without any fine-tuning. This allows us to get insights on the model itself and establish a baseline.","metadata":{}},{"cell_type":"code","source":"y_pred = predict(X_test, model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.813522Z","iopub.status.idle":"2024-09-03T10:02:07.813993Z","shell.execute_reply.started":"2024-09-03T10:02:07.813745Z","shell.execute_reply":"2024-09-03T10:02:07.813769Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the following cell, we evaluate the results. There is little to be said, it is performing really terribly because the 7b-hf model tends to just predict a neutral sentiment and seldom it detects positive or negative sentiment.","metadata":{}},{"cell_type":"code","source":"evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.815710Z","iopub.status.idle":"2024-09-03T10:02:07.816074Z","shell.execute_reply.started":"2024-09-03T10:02:07.815891Z","shell.execute_reply":"2024-09-03T10:02:07.815915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the next cell we set everything ready for the fine-tuning. We configures and initializes a Simple Fine-tuning Trainer (SFTTrainer) for training a large language model using the Parameter-Efficient Fine-Tuning (PEFT) method, which should save time as it operates on a reduced number of parameters compared to the model's overall size. The PEFT method focuses on refining a limited set of (additional) model parameters, while keeping the majority of the pre-trained LLM parameters fixed. This significantly reduces both computational and storage expenses. Additionally, this strategy addresses the challenge of catastrophic forgetting, which often occurs during the complete fine-tuning of LLMs.\n\nPEFTConfig:\n\nThe peft_config object specifies the parameters for PEFT. The following are some of the most important parameters:\n\n* lora_alpha: The learning rate for the LoRA update matrices.\n* lora_dropout: The dropout probability for the LoRA update matrices.\n* r: The rank of the LoRA update matrices.\n* bias: The type of bias to use. The possible values are none, additive, and learned.\n* task_type: The type of task that the model is being trained for. The possible values are CAUSAL_LM and MASKED_LM.\n\nTrainingArguments:\n\nThe training_arguments object specifies the parameters for training the model. The following are some of the most important parameters:\n\n* output_dir: The directory where the training logs and checkpoints will be saved.\n* num_train_epochs: The number of epochs to train the model for.\n* per_device_train_batch_size: The number of samples in each batch on each device.\n* gradient_accumulation_steps: The number of batches to accumulate gradients before updating the model parameters.\n* optim: The optimizer to use for training the model.\n* save_steps: The number of steps after which to save a checkpoint.\n* logging_steps: The number of steps after which to log the training metrics.\n* learning_rate: The learning rate for the optimizer.\n* weight_decay: The weight decay parameter for the optimizer.\n* fp16: Whether to use 16-bit floating-point precision.\n* bf16: Whether to use BFloat16 precision.\n* max_grad_norm: The maximum gradient norm.\n* max_steps: The maximum number of steps to train the model for.\n* warmup_ratio: The proportion of the training steps to use for warming up the learning rate.\n* group_by_length: Whether to group the training samples by length.\n* lr_scheduler_type: The type of learning rate scheduler to use.\n* report_to: The tools to report the training metrics to.\n* evaluation_strategy: The strategy for evaluating the model during training.\n\nSFTTrainer:\n\nThe SFTTrainer is a custom trainer class from the PEFT library. It is used to train large language models using the PEFT method.\n\nThe SFTTrainer object is initialized with the following arguments:\n\n* model: The model to be trained.\n* train_dataset: The training dataset.\n* eval_dataset: The evaluation dataset.\n* peft_config: The PEFT configuration.\n* dataset_text_field: The name of the text field in the dataset.\n* tokenizer: The tokenizer to use.\n* args: The training arguments.\n* packing: Whether to pack the training samples.\n* max_seq_length: The maximum sequence length.\n\nOnce the SFTTrainer object is initialized, it can be used to train the model by calling the train() method","metadata":{}},{"cell_type":"code","source":"import re\n\ndef get_num_layers(model):\n    numbers = set()\n    for name, _ in model.named_parameters():\n        for number in re.findall(r'\\d+', name):\n            numbers.add(int(number))\n    return max(numbers)\n\ndef get_last_layer_linears(model):\n    names = []\n    \n    num_layers = get_num_layers(model)\n    for name, module in model.named_modules():\n        if str(num_layers) in name and not \"encoder\" in name:\n            if isinstance(module, torch.nn.Linear):\n                names.append(name)\n    return names","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.817191Z","iopub.status.idle":"2024-09-03T10:02:07.817552Z","shell.execute_reply.started":"2024-09-03T10:02:07.817374Z","shell.execute_reply":"2024-09-03T10:02:07.817392Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"Wqkv\", \"out_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"logs\",\n    num_train_epochs=4,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8, # 4\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"tensorboard\",\n    evaluation_strategy=\"epoch\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n    max_seq_length=512,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.819099Z","iopub.status.idle":"2024-09-03T10:02:07.819543Z","shell.execute_reply.started":"2024-09-03T10:02:07.819320Z","shell.execute_reply":"2024-09-03T10:02:07.819349Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following code will train the model using the trainer.train() method and then save the trained model to the trained-model directory. Using The standard GPU P100 offered by Kaggle, the training should be quite fast.","metadata":{}},{"cell_type":"code","source":"# Train model\ntrainer.train()\n\n# Save trained model\ntrainer.model.save_pretrained(\"trained-model\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.820692Z","iopub.status.idle":"2024-09-03T10:02:07.821027Z","shell.execute_reply.started":"2024-09-03T10:02:07.820859Z","shell.execute_reply":"2024-09-03T10:02:07.820876Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Afterwards, loading the TensorBoard extension and start TensorBoard, pointing to the logs/runs directory, which is assumed to contain the training logs and checkpoints for your model, will allow you to understand how the models fits during the training.","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs/runs","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.822671Z","iopub.status.idle":"2024-09-03T10:02:07.823034Z","shell.execute_reply.started":"2024-09-03T10:02:07.822839Z","shell.execute_reply":"2024-09-03T10:02:07.822857Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following code will first predict the sentiment labels for the test set using the predict() function. Then, it will evaluate the model's performance on the test set using the evaluate() function. The result now should be impressive with an overall accuracy of over 0.8 and high accuracy, precision and recall for the single sentiment labels. The prediction of the neutral label can still be improved, yet it is impressive how much could be done with little data and some fine-tuning.","metadata":{}},{"cell_type":"code","source":"y_pred = predict(X_test, model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.824958Z","iopub.status.idle":"2024-09-03T10:02:07.825371Z","shell.execute_reply.started":"2024-09-03T10:02:07.825125Z","shell.execute_reply":"2024-09-03T10:02:07.825200Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following code will create a Pandas DataFrame called evaluation containing the text, true labels, and predicted labels from the test set. This is expectially useful for understanding the errors that the fine-tuned model makes, and gettting insights on how to improve the prompt.","metadata":{}},{"cell_type":"code","source":"evaluation = pd.DataFrame({'text': X_test[\"text\"], \n                           'y_true':y_true, \n                           'y_pred': y_pred},\n                         )\nevaluation.to_csv(\"test_predictions.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:02:07.828325Z","iopub.status.idle":"2024-09-03T10:02:07.828741Z","shell.execute_reply.started":"2024-09-03T10:02:07.828560Z","shell.execute_reply":"2024-09-03T10:02:07.828579Z"},"trusted":true},"outputs":[],"execution_count":null}]}